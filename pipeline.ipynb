{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrate data from S3 to Redshift with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When interacting with AWS from a Jupyter notebook or python code, it is a good practise to store relevant data that allow to communicate with the cloud in a separate config file. In this tutorial, that file is called \"dl.cfg\" and is store in the same location as the current jupyter notebook.\n",
    "The file contains three sections:\n",
    "- AWS credentials (access key ID and secret access key) needed to programmatically access AWS\n",
    "- names that will be used to create IAM role and IAM policy\n",
    "- settings that will be used to create the Redshift cluster\n",
    "- S3 location of the dataset to migrate into Redshift\n",
    "\n",
    "As a first step, let's extract some of the above mentioned parameters from \"dl.cfg\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# Read AWS credentials from the config file\n",
    "cfg_data = configparser.ConfigParser()\n",
    "cfg_data.read('dl.cfg')    \n",
    "\n",
    "# Save AWS credentials\n",
    "access_key_id     = cfg_data[\"AWS\"][\"access_key_id\"]\n",
    "secret_access_key = cfg_data[\"AWS\"][\"secret_access_key\"]\n",
    "\n",
    "# Save IAM role and IAM policy data\n",
    "role_name          = cfg_data[\"IAM\"][\"role_name\"]\n",
    "policy_name        = cfg_data[\"IAM\"][\"policy_name\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to allow Redshift reading data from S3, an Identity Access Management (IAM) role should be created. This role will allow AWS services to be called on behalf of the user. AWS IAM services can be accessed by python SDK Boto3 using a specific client. In the code below, the following operations will be executed:\n",
    "- define client to control IAM\n",
    "- check if any role with the name defined in the config file already exists and (if it does) delete it\n",
    "- create a new role destined to Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role named 'Redshift_access_S3bucket' already exists\n",
      "Role named 'Redshift_access_S3bucket' has been deleted\n",
      "Role named 'Redshift_access_S3bucket' has been created\n",
      "Role 'Redshift_access_S3bucket's ARN is: 'arn:aws:iam::341370630698:role/Redshift_access_S3bucket'\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Create IAM client for region us-east-1 feeding AWS credentials extracted from the config.json file\n",
    "iam = boto3.client(\n",
    "    \"iam\",\n",
    "    region_name = \"us-east-1\",\n",
    "    aws_access_key_id = access_key_id,\n",
    "    aws_secret_access_key = secret_access_key\n",
    ")\n",
    "\n",
    "# Try to delete the existing role with the same name, if exists\n",
    "try:\n",
    "    role = iam.get_role(\n",
    "        RoleName = role_name\n",
    "    )\n",
    "    print(\"Role named '{}' already exists\".format(role_name))\n",
    "\n",
    "    # Extract all the attached policies to the existing role\n",
    "    attached_policies = iam.list_attached_role_policies(\n",
    "        RoleName = role_name\n",
    "    )[\n",
    "        \"AttachedPolicies\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Iterate over all attached policies\n",
    "    for attached_policy in attached_policies:\n",
    "\n",
    "        # Extract attached policy ARN\n",
    "        attached_policy_arn = attached_policy[\n",
    "            \"PolicyArn\"\n",
    "        ]\n",
    "\n",
    "        # Detach policy from role\n",
    "        iam.detach_role_policy(\n",
    "            RoleName = role_name,\n",
    "            PolicyArn = attached_policy_arn\n",
    "        )\n",
    "\n",
    "    # Delete role\n",
    "    try:\n",
    "        delete_role = iam.delete_role(\n",
    "            RoleName = role_name\n",
    "        )\n",
    "        print(\"Role named '{}' has been deleted\".format(role_name))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "# Create IAM role\n",
    "try:\n",
    "    role = iam.create_role(\n",
    "        RoleName = role_name,\n",
    "        Description = \"Allows Redshift cluster to call AWS services on behalf of the user\",\n",
    "        AssumeRolePolicyDocument = json.dumps(\n",
    "            {\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Action\": \"sts:AssumeRole\",\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\n",
    "                            \"Service\": \"redshift.amazonaws.com\"\n",
    "                        }\n",
    "                     }\n",
    "                ],\n",
    "                \"Version\": \"2012-10-17\"\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    print(\"Role named '{}' has been created\".format(role_name))\n",
    "\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    " \n",
    "# Extract role ARN\n",
    "role_arn = iam.get_role(\n",
    "    RoleName = role_name\n",
    ")[\"Role\"][\"Arn\"]\n",
    "print(\"Role '{}'s ARN is: '{}'\".format(role_name, role_arn))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An IAM role does not grant itself permission to access specific AWS services. What determines which specific services are accessible is defined by an IAM policy. IAM policies are written in JSON and basically consist of a list of statements; each statement defines one or more actions, an effect (Allow or Deny), and a resource which the statement is applied to.\n",
    "\n",
    "In the code below, the following operations will be executed:\n",
    "- check if a policy with the name defined in the config file already exists\n",
    "- if a policy already exists, detach the policy from all the role it is attached to\n",
    "- delete all versions of the policy (including the default version)\n",
    "- create a new policy\n",
    "- attach the policy to the role created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy named 'read_list_target_bucket' already exists\n",
      "Policy with ARN 'arn:aws:iam::341370630698:policy/read_list_target_bucket' detached from role 'Redshift_access_S3bucket'\n",
      "Policy with ARN 'arn:aws:iam::341370630698:policy/read_list_target_bucket' deleted\n",
      "Policy named 'read_list_target_bucket' created\n",
      "Policy named 'read_list_target_bucket' has ARN 'arn:aws:iam::341370630698:policy/read_list_target_bucket'\n",
      "Policy named 'read_list_target_bucket' attached to role 'Redshift_access_S3bucket'\n"
     ]
    }
   ],
   "source": [
    "# Check if policy with the wanted name already exists\n",
    "try:\n",
    "    policies = iam.list_policies()[\"Policies\"]\n",
    "    policy_exists = False\n",
    "    for policy in policies:\n",
    "        if policy[\"PolicyName\"] == policy_name:\n",
    "            existing_policy_arn = policy[\"Arn\"]\n",
    "            policy_exists = True\n",
    "            break          \n",
    "except:\n",
    "    None\n",
    "\n",
    "# If a policy with the same name already exists, delete it\n",
    "if policy_exists:\n",
    "    print(\"Policy named '{}' already exists\".format(policy_name))\n",
    "    \n",
    "    # Extract all roles\n",
    "    roles = iam.list_roles()[\"Roles\"]\n",
    "    \n",
    "    # Iterate over all the roles\n",
    "    for role in roles:\n",
    "        \n",
    "        # Extract role name\n",
    "        existing_role_name = role[\"RoleName\"]\n",
    "        \n",
    "        # Extract all the attached policy to the role\n",
    "        attached_policies = iam.list_attached_role_policies(\n",
    "            RoleName = existing_role_name\n",
    "        )[\"AttachedPolicies\"]\n",
    "        \n",
    "        # Iterate over all the attached policies\n",
    "        for attached_policy in attached_policies:\n",
    "\n",
    "            # Extract attached policy ARN\n",
    "            attached_policy_arn = attached_policy[\"PolicyArn\"]\n",
    "\n",
    "            # Checking if the policy correspond to the wanted one\n",
    "            if attached_policy_arn == existing_policy_arn:\n",
    "                \n",
    "                # Detach policy from role\n",
    "                iam.detach_role_policy(\n",
    "                    RoleName = existing_role_name,\n",
    "                    PolicyArn = attached_policy_arn\n",
    "                )\n",
    "                \n",
    "                print(\"Policy with ARN '{}' detached from role '{}'\".format(policy_arn, existing_role_name))\n",
    "    \n",
    "    # Extract all the policy versions\n",
    "    policy_versions = iam.list_policy_versions(\n",
    "        PolicyArn = existing_policy_arn\n",
    "    )[\"Versions\"]\n",
    "    \n",
    "    # Iterate over all the policy versions\n",
    "    for policy_version in policy_versions:\n",
    "        \n",
    "        # Skip the version if it is a default version\n",
    "        if policy_version[\"IsDefaultVersion\"]:\n",
    "            continue\n",
    "          \n",
    "        # Extract policy ID\n",
    "        version_id = policy_version[\"VersionId\"]\n",
    "        \n",
    "        # Delete policy version\n",
    "        iam.delete_policy_version(\n",
    "            PolicyArn = existing_policy_arn,\n",
    "            VersionId = version_id\n",
    "        )\n",
    "        print(\"Policy with ARN '{}', version_ID '{}' deleted\".format(existing_policy_arn, version_id))\n",
    "    \n",
    "    # Delete default version of the policy\n",
    "    iam.delete_policy(\n",
    "        PolicyArn = existing_policy_arn\n",
    "    )\n",
    "    print(\"Policy with ARN '{}' deleted\".format(existing_policy_arn))\n",
    "    \n",
    "else:\n",
    "    print(\"Policy named '{}' does not exists\".format(policy_name))\n",
    " \n",
    "# Create policy \n",
    "try:\n",
    "    policy = iam.create_policy(\n",
    "        PolicyName = policy_name,\n",
    "        Description = \"Allow to list and access content of the target bucket 'data-to-migrate'\",\n",
    "        PolicyDocument = json.dumps(\n",
    "            {\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Action\": [\n",
    "                            \"s3:ListBucket\"\n",
    "                        ],\n",
    "                        \"Resource\": [\n",
    "                            \"arn:aws:s3:::data-to-migrate\"\n",
    "                        ]\n",
    "                    },\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Action\": [\n",
    "                            \"s3:PutObject\",\n",
    "                            \"s3:GetObject\",\n",
    "                            \"s3:DeleteObject\"\n",
    "                        ],\n",
    "                        \"Resource\": [\n",
    "                            \"arn:aws:s3:::data-to-migrate/*\"\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    print(\"Policy named '{}' created\".format(policy_name))\n",
    "    policy_arn = policy[\"Policy\"][\"Arn\"]\n",
    "    print(\"Policy named '{}' has ARN '{}'\".format(policy_name, policy_arn))\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "# Attach policy to IAM role\n",
    "try:\n",
    "    attachment = iam.attach_role_policy(\n",
    "        RoleName = role_name,\n",
    "        PolicyArn = policy_arn\n",
    "    )\n",
    "    print(\"Policy named '{}' attached to role '{}'\".format(policy_name, role_name))\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to what we have done for IAM, a Redshift client needs to be defined in order to control Redshift using python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Redshift client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Redshift client\n",
    "redshift = boto3.client(\n",
    "    \"redshift\",\n",
    "    region_name = \"us-east-1\",\n",
    "    aws_access_key_id = access_key_id,\n",
    "    aws_secret_access_key = secret_access_key\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Redshift related parameters from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# Read AWS credentials from the config file\n",
    "cfg_data = configparser.ConfigParser()\n",
    "cfg_data.read('dl.cfg') \n",
    "\n",
    "# Save Redshift cluster\n",
    "cluster_identifier = cfg_data[\"Redshift\"][\"cluster_identifier\"]\n",
    "cluster_type       = cfg_data[\"Redshift\"][\"cluster_type\"]\n",
    "node_type          = cfg_data[\"Redshift\"][\"node_type\"]\n",
    "username           = cfg_data[\"Redshift\"][\"username\"]\n",
    "password           = cfg_data[\"Redshift\"][\"password\"]\n",
    "database_name      = cfg_data[\"Redshift\"][\"database_name\"]\n",
    "port               = cfg_data[\"Redshift\"][\"port\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we cannot have more than one Redshift cluster with the same name, the following operations need to be performed to create a Redshift cluster:\n",
    "- check if a Redshif cluster with the wanted name already exists\n",
    "- if it does, delete it\n",
    "- create a new cluster\n",
    "- extract relevant cluster information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cluster named my-redshift-cluster does not exist\n"
     ]
    }
   ],
   "source": [
    "# Delete Redshift cluster with the same name if it exists\n",
    "   \n",
    "try:\n",
    "    # Delete Cluster\n",
    "    redshift.delete_cluster(\n",
    "        ClusterIdentifier = cluster_identifier,\n",
    "        SkipFinalClusterSnapshot = True,\n",
    "    )\n",
    "\n",
    "    print(\"A cluster named '{}' already exists\".format(cluster_identifier))\n",
    "    print(\"Deleting existing cluster named '{}'...\".format(cluster_identifier))\n",
    "\n",
    "\n",
    "    # Wait for the cluster status change to deleted\n",
    "    delete_waiter = redshift.get_waiter(\"cluster_deleted\")\n",
    "    delete_waiter.wait(\n",
    "        ClusterIdentifier = cluster_identifier,\n",
    "        WaiterConfig = {\n",
    "            \"Delay\": 30,\n",
    "            \"MaxAttempts\": 20\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"Existing cluster named '{}' deleted\".format(cluster_identifier))\n",
    "\n",
    "except:\n",
    "    print(\"A cluster named '{}' does not exist\".format(cluster_identifier))  \n",
    "\n",
    "\n",
    "# Create Redshift cluster\n",
    "try:\n",
    "    cluster = redshift.create_cluster(\n",
    "        DBName = database_name, # (OPT) name of the first database to create when the cluster is created\n",
    "        ClusterIdentifier = cluster_identifier, # (REQ) name of the cluster\n",
    "        ClusterType = cluster_type, # (OPT) singlenode vs multinode\n",
    "        NodeType = node_type, # (REQ) type of node\n",
    "        MasterUsername = username, # (REQ) username\n",
    "        MasterUserPassword = password, # (REQ) password\n",
    "        Port = port, # port number on which the cluster accepts inbound connections\n",
    "        IamRoles = [role_arn] # list of role Arns defining how Redshift can access other AWS services\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"Creating new cluster named '{}'...\".format(cluster_identifier))\n",
    "\n",
    "# Wait for the new cluster status change to available\n",
    "create_waiter = redshift.get_waiter(\"cluster_available\")\n",
    "create_waiter.wait(\n",
    "        ClusterIdentifier = cluster_identifier,\n",
    "        WaiterConfig = {\n",
    "            \"Delay\": 30,\n",
    "            \"MaxAttempts\": 20\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"New cluster named '{}' created and available\").format(cluster_identifier)\n",
    "\n",
    "# Extract cluster info\n",
    "cluster_info = redshift.describe_clusters(\n",
    "        ClusterIdentifier = cluster_identifier\n",
    "    )[\"Clusters\"][0]\n",
    "\n",
    "cluster_endpoint = cluster_info[\"endpoint\"]\n",
    "vpc_security_group_id = cluster_info[\"vpc_security_group_id\"]\n",
    "print(\"Cluster '{}' endpoint is '{}'\").format(cluster_identifier, cluster_endpoint)\n",
    "print(\"Cluster '{}''s VPC security group ID is '{}'\").format(cluster_identifier, vpc_security_group_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to communicate with the database hosted on the Redshift cluster through Boto3, it is necessary to authorize an ingress through port 5439 into the cluster VPC security group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule requested already exists\n"
     ]
    }
   ],
   "source": [
    "# Set a VPC security group rule to allow a connection through port 5439\n",
    "\n",
    "try:\n",
    "    # Define EC2 resource\n",
    "    ec2 = boto3.resource(\n",
    "        \"ec2\",\n",
    "        region_name = \"us-east-1\",\n",
    "        aws_access_key_id = access_key_id,\n",
    "        aws_secret_access_key = secret_access_key\n",
    "    )\n",
    "\n",
    "    # Extract security group for the VPC\n",
    "    vpc_sg = ec2.SecurityGroup(id = vpc_security_group_id)\n",
    "    \n",
    "    # Authorize connection to the VPC\n",
    "    vpc_sg.authorize_ingress(\n",
    "        GroupName = vpc_sg.group_name,\n",
    "        CidrIp = \"0.0.0.0/0\",\n",
    "        IpProtocol = \"TCP\",\n",
    "        FromPort = 5439,\n",
    "        ToPort = 5439\n",
    "    )\n",
    "    print(\"Ingress to the VPC authorized\")\n",
    "    \n",
    "except Exception as e:\n",
    "    \n",
    "    # Check if the error is a duplication error\n",
    "    if \"InvalidPermission.Duplicate\" in str(e):\n",
    "        print(\"Rule requested already exists\")\n",
    "    else:\n",
    "        print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library psycopg2 allows executing Postgres SQL queries on a database. In order to connect to the database, a connection string of the type \"postgresql://MasterUsername:MasterUserPassword@ClusterEndpoint:DatabasePort,DatabaseName\" is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Connected: ldefra-user@dev'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "%load_ext sql\n",
    "conn_string = \"postgresql://{}:{}@{}:{}/{}\".format(\n",
    "    \"ldefra-user\",\n",
    "    \"MyPassword2020\",\n",
    "    cluster_endpoint,\n",
    "    5439,\n",
    "    \"dev\"\n",
    ")\n",
    "%sql $conn_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to facilitate query execution on the Postgres database, let's create a function that establishes a connection to the database, execute the provided query and close the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql(sql_query, conn_string, print_results = False):\n",
    "    \"\"\"Execute a SQL query on the database associated with\n",
    "       a connection string\n",
    "    \n",
    "    Parameters:\n",
    "    - sql_query : str\n",
    "        SQL query to execute\n",
    "    - conn_string : str\n",
    "        connection string of the format 'postgresql://MasterUsername:MasterUserPassword@ClusterEndpoint:DatabasePort,DatabaseName'\n",
    "    - print_results : bool\n",
    "        select if to print query results or not\n",
    "    \"\"\"\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = psycopg2.connect(conn_string)\n",
    "    \n",
    "    # Define cursor\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Execute query\n",
    "    cur.execute(sql_query)\n",
    "    conn.commit()\n",
    "    if print_results:\n",
    "        print(cur.fetchall()\n",
    "\n",
    "    # Close cursor\n",
    "    cur.close()\n",
    "    \n",
    "    # Close connection\n",
    "    conn.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating a table that can contain data coming from S3, these data should be explored to assess their structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the exploration run below, the dataset is structured into 5 columns:\n",
    "- petal length\n",
    "- petal width\n",
    "- sepal length\n",
    "- sepal width\n",
    "- species\n",
    "\n",
    "The columns listed above can be transferred to a Postgres table using the following datatypes formats:\n",
    "- petal length (NUMERIC)\n",
    "- petal width (NUMERIC)\n",
    "- sepal length (NUMERIC)\n",
    "- sepal width (NUMERIC)\n",
    "- species (VARCHAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (InvalidAccessKeyId) when calling the GetObject operation: The AWS Access Key Id you provided does not exist in our records.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-a6ea0e9ce62c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m obj = s3.get_object(\n\u001b[1;32m      8\u001b[0m     \u001b[0mBucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data-to-migrate\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mKey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"iris_dataset.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (InvalidAccessKeyId) when calling the GetObject operation: The AWS Access Key Id you provided does not exist in our records."
     ]
    }
   ],
   "source": [
    "# Define S3 client\n",
    "s3 = boto3.client(\n",
    "    \"s3\"\n",
    ")\n",
    "\n",
    "# Get object containing file to be staged\n",
    "obj = s3.get_object(\n",
    "    Bucket = \"data-to-migrate\",\n",
    "    Key = \"iris_dataset.csv\"\n",
    ")\n",
    "\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "# Print colummns info for the dataset\n",
    "pd.read_csv(io.BytesIO(obj[\"Body\"].read())).info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the information of the source dataset, a new table can be created in database running on the Redshift cluster. The code below deletes a table if already exists, creates a new table and finally copy the data into the new table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete existing table named \"iris\"\n",
    "sql_query = \"\"\"DROP TABLE IF EXISTS iris\"\"\"\n",
    "execute_sql(sql_query, conn_string)\n",
    "\n",
    "print(\"Table deleted if existing\")\n",
    "\n",
    "# Create a new table named \"iris\"\n",
    "sql_query = \"\"\"CREATE TABLE IF NOT EXISTS iris\n",
    "               (\n",
    "               sepal_length NUMERIC,\n",
    "               sepal_width NUMERIC,\n",
    "               petal_length NUMERIC,\n",
    "               petal_width NUMERIC,\n",
    "               species VARCHAR\n",
    "               )\n",
    "            \"\"\"\n",
    "execute_sql(sql_query, conn_string)\n",
    "\n",
    "print(\"Table created\")\n",
    "\n",
    "# Define S3 source file path\n",
    "file_path = \"s3://data-to-migrate/iris_dataset.csv\"\n",
    "\n",
    "# # Copy data\n",
    "sql_query = \"\"\"\n",
    "    COPY iris\n",
    "    FROM '{}'\n",
    "    IAM_ROLE '{}' \n",
    "    csv\n",
    "    IGNOREHEADER 1\n",
    "    ;\n",
    "\"\"\".format(file_path, role_arn)\n",
    "execute_sql(sql_query, conn_string)\n",
    "\n",
    "print(\"Data copied\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "sql_query = \"\"\"SELECT *\n",
    "               FROM iris\n",
    "               LIMIT 5\n",
    "            \"\"\"\n",
    "execute_sql(\n",
    "    sql_query,\n",
    "    conn_string,\n",
    "    True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, we want to propose the code needed to delete the Redshift cluster. It is adviced to delete the Redshift cluster if not being used in order to minimize AWS cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Delete Cluster\n",
    "    redshift.delete_cluster(\n",
    "        ClusterIdentifier = cluster_identifier,\n",
    "        SkipFinalClusterSnapshot = True,\n",
    "    )\n",
    "\n",
    "    print(\"A cluster named {} exists\".format(cluster_identifier))\n",
    "    print(\"Deleting existing cluster named {}...\".format(cluster_identifier))\n",
    "\n",
    "\n",
    "    # Wait for the cluster status change to deleted\n",
    "    delete_waiter = redshift.get_waiter(\"cluster_deleted\")\n",
    "    delete_waiter.wait(\n",
    "        ClusterIdentifier = cluster_identifier,\n",
    "        WaiterConfig = {\n",
    "            \"Delay\": 30,\n",
    "            \"MaxAttempts\": 20\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"Cluster named {} deleted\".format(cluster_identifier))\n",
    "\n",
    "except:\n",
    "    print(\"A cluster named {} does not exist\".format(cluster_identifier))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
